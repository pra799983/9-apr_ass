{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "922c822d-492c-429b-afd7-9ff66e065ee0",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q1. What is Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fe87c964-680d-468b-8031-5817c4e3f306",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is a fundamental concept in probability theory and statistics, named after the Reverend Thomas Bayes. It provides a way to \n",
    "update our beliefs or knowledge about an event or hypothesis in the light of new evidence. The theorem mathematically calculates the\n",
    "conditional probability of an event or hypothesis given some observed evidence.\n",
    "\n",
    "The theorem is stated as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "where:\n",
    "\n",
    "P(A|B) is the probability of event or hypothesis A given evidence B. This is called the posterior probability.\n",
    "P(B|A) is the probability of observing evidence B given that event or hypothesis A is true. This is called the likelihood.\n",
    "P(A) is the probability of event or hypothesis A being true without considering the evidence B. This is called the prior probability.\n",
    "P(B) is the probability of observing the evidence B, irrespective of the event or hypothesis. This is called the marginal likelihood or the\n",
    "evidence.\n",
    "In simpler terms, Bayes' theorem allows us to update our prior beliefs (prior probability) about an event or hypothesis based on new \n",
    "observed evidence. The theorem considers how likely the evidence would be if the event or hypothesis were true (likelihood) and combines \n",
    "it with the initial belief to calculate the updated probability (posterior probability)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a98490a-4efb-441c-81e9-2f74773f4bc9",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q2. What is the formula for Bayes' theorem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4098b578-7b94-430a-88d6-ebdef8cc0a7c",
   "metadata": {},
   "outputs": [],
   "source": [
    "The theorem is stated as follows:\n",
    "\n",
    "P(A|B) = (P(B|A) * P(A)) / P(B)\n",
    "\n",
    "P(A|B) is the probability of event or hypothesis A given evidence B. This is called the posterior probability.\n",
    "P(B|A) is the probability of observing evidence B given that event or hypothesis A is true. This is called the likelihood.\n",
    "P(A) is the probability of event or hypothesis A being true without considering the evidence B. This is called the prior probability.\n",
    "P(B) is the probability of observing the evidence B, irrespective of the event or hypothesis. This is called the marginal likelihood or the\n",
    "evidence."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "caff2e60-00ce-472a-a1b9-8c20f4718bff",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q3. How is Bayes' theorem used in practice?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4e97313-b5f6-411f-9d82-33527b0317e8",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is used in various fields to make informed decisions, perform statistical inference, and update beliefs based on new\n",
    "evidence. Here are a few practical applications of Bayes' theorem:\n",
    "\n",
    "Bayesian Statistics: Bayes' theorem is at the core of Bayesian statistics, a statistical framework that combines prior knowledge or beliefs \n",
    "ith observed data to obtain posterior probabilities. It allows for the updating of probabilities as new data is acquired, providing a \n",
    "flexible and coherent approach to statistical analysis.\n",
    "\n",
    "Medical Diagnosis: Bayes' theorem plays a crucial role in medical diagnosis. Doctors use prior probabilities (based on their knowledge and \n",
    "experience) and combine them with test results or symptoms to determine the probability of a particular disease or condition. This helps in \n",
    "making accurate diagnoses and treatment decisions.\n",
    "\n",
    "Spam Filtering: Email providers often use Bayesian spam filters to classify incoming emails as either spam or legitimate. These filters \n",
    "calculate the probability that an email is spam or not spam based on various features (e.g., keywords, sender information) and update the \n",
    "probabilities as new emails are received. Bayes' theorem allows for the continual adjustment of spam classification based on observed\n",
    "patterns.\n",
    "\n",
    "Pattern Recognition: Bayes' theorem is employed in pattern recognition tasks, such as image or speech recognition. It helps in classifying \n",
    "or recognizing patterns by combining prior probabilities of different classes with observed features or measurements.\n",
    "\n",
    "Machine Learning: Bayes' theorem is a fundamental component of several machine learning algorithms, particularly in the field of Bayesian\n",
    "machine learning. Bayesian models leverage Bayes' theorem to update prior beliefs about model parameters based on observed data, allowing \n",
    "for robust learning and prediction."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41a808c1-76d4-4940-adcf-06a022a70f76",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q4. What is the relationship between Bayes' theorem and conditional probability?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3676aa57-2b0b-45ae-96fd-648a4603f558",
   "metadata": {},
   "outputs": [],
   "source": [
    "Bayes' theorem is closely related to conditional probability. In fact, Bayes' theorem can be derived from the definition of conditional \n",
    "probability.\n",
    "\n",
    "Conditional probability measures the probability of an event A occurring given that another event B has already occurred. It is denoted as \n",
    "P(A|B) and read as \"the probability of A given B.\" It can be calculated using the formula:\n",
    "\n",
    "P(A|B) = P(A ∩ B) / P(B)\n",
    "\n",
    "where P(A ∩ B) represents the probability of both A and B occurring simultaneously, and P(B) is the probability of event B occurring.\n",
    "\n",
    "Bayes' theorem builds upon this concept of conditional probability. It provides a way to reverse the conditioning, allowing us to calculate \n",
    "the probability of event B given event A. The theorem states:\n",
    "\n",
    "P(B|A) = (P(A|B) * P(B)) / P(A)\n",
    "\n",
    "Here, P(B|A) represents the probability of event B occurring given that event A has occurred. This is the posterior probability. P(A|B) is \n",
    "the likelihood, the probability of event A given event B. P(B) is the prior probability, the probability of event B occurring without \n",
    "considering event A. P(A) is the marginal likelihood or evidence, the probability of event A occurring irrespective of event B."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "faded8bd-4e5d-4901-8934-3bc6131dacd5",
   "metadata": {},
   "outputs": [],
   "source": [
    "Q5. How do you choose which type of Naive Bayes classifier to use for any given problem?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "815677a5-7c56-4277-9658-d5171f151d94",
   "metadata": {},
   "outputs": [],
   "source": [
    "When choosing a type of Naive Bayes classifier for a given problem, several factors should be considered. The choice depends on the nature\n",
    "of the problem, the characteristics of the data, and certain assumptions made by different Naive Bayes variants.\n",
    "Here are some considerations to guide your decision:\n",
    "\n",
    "Multinomial Naive Bayes:\n",
    "\n",
    "Suitable for problems with discrete features, such as word frequencies in text classification.\n",
    "Assumes that features follow a multinomial distribution.\n",
    "Works well with feature counts or frequency-based representations.\n",
    "Gaussian Naive Bayes:\n",
    "\n",
    "Appropriate for problems with continuous features that can be modeled by a Gaussian (normal) distribution.\n",
    "Assumes that each class has a specific Gaussian distribution for each feature.\n",
    "Works well with numeric data and when features are assumed to be independent.\n",
    "Bernoulli Naive Bayes:\n",
    "\n",
    "Suitable for problems with binary features, such as presence/absence or yes/no representations.\n",
    "Assumes that features are conditionally independent and follow a Bernoulli distribution.\n",
    "Works well with binary or boolean data.\n",
    "Complement Naive Bayes:\n",
    "\n",
    "Designed to address imbalanced class distributions.\n",
    "Particularly effective when there is a large imbalance between the number of instances in different classes.\n",
    "Can provide more accurate results than other Naive Bayes variants for imbalanced datasets."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70163aeb-4064-4923-97a6-58ec9bc4175b",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Q6. Assignment:\n",
    "# You have a dataset with two features, X1 and X2, and two possible classes, A and B. You want to use Naive\n",
    "# Bayes to classify a new instance with features X1 = 3 and X2 = 4. The following table shows the frequency of\n",
    "# each feature value for each class:\n",
    "# Class X1=1 X1=2 X1=3 X2=1 X2=2 X2=3 X2=4\n",
    "# A 3 3 4 4 3 3 3\n",
    "# B 2 2 1 2 2 2 3\n",
    "# Assuming equal prior probabilities for each class, which class would Naive Bayes predict the new instance\n",
    "# to belong to?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8283e38d-6f67-477a-a9b4-cb8536eaec6a",
   "metadata": {},
   "outputs": [],
   "source": [
    "# To predict the class for the new instance (X1 = 3, X2 = 4) using Naive Bayes, we need to calculate the posterior probabilities for each \n",
    "# class and choose the class with the highest probability. \n",
    "\n",
    "# Calculate the prior probabilities:\n",
    "# Since equal prior probabilities are assumed for each class, P(A) = P(B) = 0.5.\n",
    "\n",
    "# Calculate the likelihoods:\n",
    "# For Class A:\n",
    "\n",
    "# P(X1 = 3 | A) = 4/13\n",
    "# P(X2 = 4 | A) = 3/13\n",
    "# For Class B:\n",
    "\n",
    "# P(X1 = 3 | B) = 1/7\n",
    "# P(X2 = 4 | B) = 3/7\n",
    "# Calculate the evidence:\n",
    "# Since the prior probabilities are equal, we do not need to calculate the evidence (marginal likelihood) P(X1 = 3, X2 = 4) because \n",
    "# it will be the same for both classes.\n",
    "\n",
    "# Apply Bayes' theorem and calculate the posterior probabilities:\n",
    "# For Class A:\n",
    "# P(A | X1 = 3, X2 = 4) = (P(X1 = 3 | A) * P(X2 = 4 | A) * P(A)) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "# For Class B:\n",
    "# P(B | X1 = 3, X2 = 4) = (P(X1 = 3 | B) * P(X2 = 4 | B) * P(B)) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "# Since the evidence is the same for both classes, we can omit it in the comparison.\n",
    "\n",
    "# Calculate the posterior probabilities:\n",
    "# For Class A:\n",
    "# P(A | X1 = 3, X2 = 4) = (4/13 * 3/13 * 0.5) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "# For Class B:\n",
    "# P(B | X1 = 3, X2 = 4) = (1/7 * 3/7 * 0.5) / P(X1 = 3, X2 = 4)\n",
    "\n",
    "# Compare the posterior probabilities:\n",
    "# To determine the predicted class, we compare the two posterior probabilities. The class with the higher posterior probability will be\n",
    "# chosen.\n",
    "\n",
    "# Now, we need the value of P(X1 = 3, X2 = 4) to compute the exact posterior probabilities. However, since the evidence is the same for both \n",
    "# classes, it cancels out when comparing the probabilities. Therefore, we can omit it and directly compare the likelihoods multiplied by the \n",
    "# prior probabilities.\n",
    "\n",
    "# For Class A:\n",
    "# P(A | X1 = 3, X2 = 4) ∝ (4/13 * 3/13 * 0.5)\n",
    "\n",
    "# For Class B:\n",
    "# P(B | X1 = 3, X2 = 4) ∝ (1/7 * 3/7 * 0.5)\n",
    "\n",
    "# Calculating these values, we find:\n",
    "# P(A | X1 = 3, X2 = 4) ≈ 0.029\n",
    "# P(B | X1 = 3, X2 = 4) ≈ 0.024\n",
    "\n",
    "# Since P(A | X1 = 3, X2 = 4) > P(B | X1 = 3, X2 = 4), Naive Bayes would predict the new instance to belong to Class A."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f88bd626-0cd0-4f13-b159-d5e158c915cb",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2e02dcc-1559-4a24-ae6d-0b76bcb81004",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de6ae61e-0aeb-496d-bfd9-a4b5c4e6b2f3",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
